<!DOCTYPE html>
<html>

<head>
    <title>ai augmented winners</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../../styles/reset.css" />
    <link rel="stylesheet" type="text/css" href="../../styles/writingstyle.css" />
</head>

<body>
    <div id="contents">
        <br>
        <br>
        <br>
        <div id="date">2024/05/26</div>
        <br>
        <br>
        <div id="title">ai augmented winners</div>
        <br>
        <hr>
        <br>
        <br>
        <p>There is a clip of Peter Thiel's take on who stands to gain the most from the proliferation of
            generative ai.</p>
        <br>
        <br>
        <blockquote class="twitter-tweet" data-media-max-width="560">
            <p lang="en" dir="ltr">Peter Thiel: AI is bad news for people for math skills and society is going to shift
                in favor of people with strong verbal skills. <a
                    href="https://t.co/4bfiMTJrbh">pic.twitter.com/4bfiMTJrbh</a></p>&mdash; Zain Kahn (@heykahn) <a
                href="https://twitter.com/heykahn/status/1794699008885350598?ref_src=twsrc%5Etfw">May 26, 2024</a>
        </blockquote>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        <br>
        <br>
        <p>Much of the debate surrounding this topic is framed around a frequently recurring motif in this space called
            the <a href="https://roonscape.ai/p/a-song-of-shapes-and-words" target="_blank">shape-rotator vs
                wordcel</a>
            dichotomy.</p>
        <br>
        <br>
        <p>To get the first caveat aside. Most of these models currently seem to perform better within natural language
            domains (English, Japanese, etc.) than quantitative (mathematical) or procedural (programming langauge)
            domains. This likely won't be a problem for much longer as AI labs are rushing to address this gap in the market.</p>
        <br>
        <br>
        <p>Taking a step back to look at things from a more fundamental view, these models are machines that
            can perform massive amounts of data compression. Just to appreciate the scale of this compression scheme,
            the training process can involve taking tens of terabytes of data from the internet (eg. llama3 being
            trained on 15T tokens ~= 60TB, assuming each token requires 4 bytes of memory) and iteratively compressing
            them into representations that are distributed across the model's weights which can be up to a few orders of
            magnitude smaller (eg. 8B parameters ~= 16GB, assuming each parameter requires 2 bytes of memory). It
            logically follows that a more lossless "compression algorithm" translates to models that can better capture
            the underlying nuances within the <a
                href="https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/" target="_blank">data</a> they
            are trained on. We've also learned that many of our current approaches to machine learning have been
            constrained
            by <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" target="_blank">scale</a> and it
            doesn't look like our bet on this scaling thesis will be slowing down any time soon (see <a
                href="https://finance.yahoo.com/quote/NVDA/" target="_blank">$NVDA</a>).</p>
        <br>
        <br>
        <p>I find it valuable to view these models as compression machines because it helps us pose the question:
            what kind of information benefits us the most once effectively compressed?</p>
        <br>
        <br>
        <p>
            Our modern society is built upon increasingly convoluted and intertwined layers of
            abstraction that have accumulated over time. This has resulted in a large demand for domain "experts" who are hired to
            untangle this mess by parsing through a jungle of jargon and sifting through all the <a
                href="https://en.wikipedia.org/wiki/Boilerplate_text" target="_blank">boilerplate</a> before attempting to get anything done. This is a problem because there is a ton of opportunity cost associated with this
            kind of subdomain abstraction-specific specialization. It is merely treating the symptoms of
            inflammation within these industries instead of curing the disease itself. Large language models feel
            like a match made in heaven for such a problem, as they can compress these abstractions and help us 
            crunch through the layers that we find to be the most tedious. This form of automatation can be
            what saves us from drowning in the bureaucratic quicksand that pervades every industry.
        </p>
        <br>
        <br>
        <p>Circling back to the issue of whether the mathematically-inclined or the communicatively-inclined will
            prosper more from using these large language (and soon multimodal) models, I opt for a third answer that is
            somehwat of a copout. Those who will benifit the most from using these models will need to have both
            skillsets.</p>
        <br>
        <br>
        <p>One will need to have just enough understanding of the low-level math / science / engineering in their field
            so
            that they can intuively know the fundamental limitations and boundaries of what they're working on (ie.
            first principles thinking a la Elon Musk). At the same time, one will also need to have just enough verbal
            acuity and communication skills to draw up and express their high level vision of the task they want to get
            accomplished. These two skillsets will meet in the middle when it comes to sampling from the models and
            executing on their outputs as they navigate through the intermediate layers of abstraction between
            an implemented solution and the goal they have in mind.</p>
        <br>
        <br>
        <p>TLDR: Those who reap the most rewards using AI in the near future will be those who possess: </p>
        <ul>
            <p>- a technical understanding of low-level details</p>
            <p>- an ability to navigate through the mid-level abstractions by piloting these models</p>
            <p>- the verbal acuity to articulate high-level goals</p>
        </ul>
        <br>
        <br>
        <hr>
        <br>
        <br>
        <a href="../../writing.html" id="back-link"> &lt; return </a>
    </div>
</body>

</html>
